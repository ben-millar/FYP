{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class bin_data:\n",
    "    def __init__(self, env):\n",
    "        self.total_capacity = np.full(env.num_bins, 0) # Total capacity seen across all trials\n",
    "        self.capacity_used = np.full(env.num_bins, 0) # Total capacity used across all trials\n",
    "        self.steps_taken = 0 # Number of timesteps taken before trial terminated\n",
    "        self.data_points = 0 # Number of trials we've witnessed\n",
    "    \n",
    "\n",
    "    def log(self, env):\n",
    "        self.total_capacity = np.add(self.total_capacity, env.capacity)\n",
    "        self.capacity_used = np.add(self.capacity_used, env.state[:-1])\n",
    "        self.steps_taken += sum(env.logs.values())\n",
    "        self.data_points += 1\n",
    "        \n",
    "\n",
    "    def get_avg(self):\n",
    "        percentages = 100 - ((env.state/env.capacity) * 100)\n",
    "        \n",
    "        return {\n",
    "            \"steps\" : self.steps_taken / self.data_points,\n",
    "            \"utilization\" : sum(percentages) / len(percentages)\n",
    "        }\n",
    "    \n",
    "\n",
    "    def print_data(self):\n",
    "        data = control_data.get_avg()\n",
    "        print(f'Average number of steps taken: {data[\"steps\"]}')\n",
    "        print(f'Average bin utilization: {round(data[\"utilization\"], 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.7.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinPackingEnvironment1D import BinPacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BinPacking(num_bins=10, capacity=20, min_item_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 14, 19,  0, 15,  9,  3, 10,  2, 14,  7])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run baseline test (No ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_data = bin_data(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-125\n",
      "{'placed': 43, 'misplaced': 168, 'discarded': 19}\n",
      "Episode:2 Score:-312\n",
      "{'placed': 42, 'misplaced': 354, 'discarded': 43}\n",
      "Episode:3 Score:-119\n",
      "{'placed': 40, 'misplaced': 159, 'discarded': 18}\n",
      "Episode:4 Score:-560\n",
      "{'placed': 54, 'misplaced': 614, 'discarded': 58}\n",
      "Episode:5 Score:-564\n",
      "{'placed': 51, 'misplaced': 615, 'discarded': 65}\n",
      "Episode:6 Score:-449\n",
      "{'placed': 54, 'misplaced': 503, 'discarded': 64}\n",
      "Episode:7 Score:-250\n",
      "{'placed': 43, 'misplaced': 293, 'discarded': 28}\n",
      "Episode:8 Score:-255\n",
      "{'placed': 50, 'misplaced': 305, 'discarded': 46}\n",
      "Episode:9 Score:-77\n",
      "{'placed': 37, 'misplaced': 114, 'discarded': 13}\n",
      "Episode:10 Score:-659\n",
      "{'placed': 52, 'misplaced': 711, 'discarded': 69}\n",
      "Average number of steps taken: 472.5\n",
      "Average bin utilization: 96.82\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 1000\n",
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done and steps < MAX_STEPS:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "    print(env.logs)\n",
    "    control_data.log(env)\n",
    "    env.logs = { 'placed':0, 'misplaced':0, 'discarded':0 }\n",
    "\n",
    "control_data.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will throw an error if these don't exist\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BinPacking(num_bins=10, capacity=20, min_item_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_41\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 546      |\n",
      "|    ep_rew_mean     | -414     |\n",
      "| time/              |          |\n",
      "|    fps             | 1281     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 365         |\n",
      "|    ep_rew_mean          | -230        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 840         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049018357 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.0351      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.78        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 63.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 299         |\n",
      "|    ep_rew_mean          | -161        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 747         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017136946 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.01        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 40.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 270         |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 703         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014414625 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 29.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 254         |\n",
      "|    ep_rew_mean          | -109        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 683         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013538784 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 28.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 244         |\n",
      "|    ep_rew_mean          | -93.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 678         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014185113 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.08       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.63        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 233         |\n",
      "|    ep_rew_mean          | -79         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 670         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013216844 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 20.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 220         |\n",
      "|    ep_rew_mean          | -64.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009514316 |\n",
      "|    clip_fraction        | 0.0944      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.44        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 218         |\n",
      "|    ep_rew_mean          | -56.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015340541 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.64        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 15.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 215         |\n",
      "|    ep_rew_mean          | -48.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011920838 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.92        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 7.35        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1e80c5166d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO_Path = os.path.join('Training', 'Saved Models', 'Constant_PPO_Model_Discard_Penalty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-2\n",
      "{'placed': 56, 'misplaced': 58, 'discarded': 151}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 100.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 95.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 95.0%\n",
      "Total steps taken: 265\n",
      "Episode:2 Score:11\n",
      "{'placed': 51, 'misplaced': 40, 'discarded': 120}\n",
      "Bin 1: 95.0%\n",
      "Bin 2: 100.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 95.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 95.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 100.0%\n",
      "Total steps taken: 211\n",
      "Episode:3 Score:-4\n",
      "{'placed': 57, 'misplaced': 61, 'discarded': 201}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 100.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 95.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 100.0%\n",
      "Total steps taken: 319\n",
      "Episode:4 Score:20\n",
      "{'placed': 55, 'misplaced': 35, 'discarded': 49}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 95.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 95.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 95.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 95.0%\n",
      "Total steps taken: 139\n",
      "Episode:5 Score:-3\n",
      "{'placed': 50, 'misplaced': 53, 'discarded': 120}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 100.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 100.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 95.0%\n",
      "Total steps taken: 223\n",
      "Episode:6 Score:28\n",
      "{'placed': 51, 'misplaced': 23, 'discarded': 52}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 95.0%\n",
      "Bin 3: 95.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 100.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 100.0%\n",
      "Total steps taken: 126\n",
      "Episode:7 Score:23\n",
      "{'placed': 51, 'misplaced': 28, 'discarded': 94}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 95.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 95.0%\n",
      "Bin 7: 100.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 95.0%\n",
      "Total steps taken: 173\n",
      "Episode:8 Score:22\n",
      "{'placed': 52, 'misplaced': 30, 'discarded': 134}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 95.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 100.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 95.0%\n",
      "Total steps taken: 216\n",
      "Episode:9 Score:28\n",
      "{'placed': 51, 'misplaced': 23, 'discarded': 92}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 95.0%\n",
      "Bin 3: 95.0%\n",
      "Bin 4: 95.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 100.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 95.0%\n",
      "Total steps taken: 166\n",
      "Episode:10 Score:18\n",
      "{'placed': 54, 'misplaced': 36, 'discarded': 104}\n",
      "Bin 1: 100.0%\n",
      "Bin 2: 95.0%\n",
      "Bin 3: 100.0%\n",
      "Bin 4: 100.0%\n",
      "Bin 5: 100.0%\n",
      "Bin 6: 100.0%\n",
      "Bin 7: 100.0%\n",
      "Bin 8: 100.0%\n",
      "Bin 9: 100.0%\n",
      "Bin 10: 100.0%\n",
      "Total steps taken: 194\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 1000\n",
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done and steps < MAX_STEPS:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "    print_metrics(env)\n",
    "    env.logs = { 'placed':0, 'misplaced':0, 'discarded':0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

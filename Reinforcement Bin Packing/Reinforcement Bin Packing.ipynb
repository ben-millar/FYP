{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class bin_data:\n",
    "    def __init__(self, env):\n",
    "        self.total_capacity = np.full(env.num_bins, 0) # Total capacity seen across all trials\n",
    "        self.capacity_used = np.full(env.num_bins, 0) # Total capacity used across all trials\n",
    "        self.steps_taken = 0 # Number of timesteps taken before trial terminated\n",
    "        self.placed = 0 # Number of items correctly placed\n",
    "        self.misplaced = 0\n",
    "        self.discarded = 0\n",
    "        self.data_points = 0 # Number of trials we've witnessed\n",
    "    \n",
    "\n",
    "    def log(self, env):\n",
    "        self.total_capacity = np.add(self.total_capacity, env.capacity)\n",
    "        self.capacity_used = np.add(self.capacity_used, env.state[:-1])\n",
    "        self.steps_taken += sum(env.logs.values())\n",
    "        self.placed += env.logs[\"placed\"]\n",
    "        self.misplaced += env.logs[\"misplaced\"]\n",
    "        self.discarded += env.logs[\"discarded\"]\n",
    "        self.data_points += 1\n",
    "        \n",
    "\n",
    "    def get_avg(self):\n",
    "        percentages = 100 - ((env.state/env.capacity) * 100)\n",
    "        \n",
    "        return {\n",
    "            \"steps\" : self.steps_taken / self.data_points,\n",
    "            \"utilization\" : sum(percentages) / len(percentages),\n",
    "            \"accuracy\" : (self.placed / (self.placed + self.misplaced)) * 100\n",
    "        }\n",
    "    \n",
    "\n",
    "    def print_data(self):\n",
    "        data = self.get_avg()\n",
    "        print(f'Average number of steps taken: {data[\"steps\"]}')\n",
    "        print(f'Average bin utilization: {round(data[\"utilization\"], 2)}%')\n",
    "        print(f'Accuracy: {round(data[\"accuracy\"], 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.7.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BinPackingEnvironment1D import BinPacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BinPacking(num_bins=10, capacity=20, min_item_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 14, 19,  0, 15,  9,  3, 10,  2, 14,  7])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run baseline test (No ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_data = bin_data(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-192\n",
      "{'placed': 44, 'misplaced': 236, 'discarded': 26}\n",
      "Episode:2 Score:-235\n",
      "{'placed': 47, 'misplaced': 282, 'discarded': 37}\n",
      "Episode:3 Score:-423\n",
      "{'placed': 46, 'misplaced': 469, 'discarded': 65}\n",
      "Episode:4 Score:-58\n",
      "{'placed': 40, 'misplaced': 98, 'discarded': 12}\n",
      "Episode:5 Score:-462\n",
      "{'placed': 44, 'misplaced': 506, 'discarded': 69}\n",
      "Episode:6 Score:-549\n",
      "{'placed': 48, 'misplaced': 597, 'discarded': 54}\n",
      "Episode:7 Score:-231\n",
      "{'placed': 46, 'misplaced': 277, 'discarded': 31}\n",
      "Episode:8 Score:-77\n",
      "{'placed': 42, 'misplaced': 119, 'discarded': 12}\n",
      "Episode:9 Score:-208\n",
      "{'placed': 56, 'misplaced': 264, 'discarded': 44}\n",
      "Episode:10 Score:-130\n",
      "{'placed': 54, 'misplaced': 184, 'discarded': 27}\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 1000\n",
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done and steps < MAX_STEPS:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "    print(env.logs)\n",
    "    control_data.log(env)\n",
    "    env.logs = { 'placed':0, 'misplaced':0, 'discarded':0 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will throw an error if these don't exist\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BinPacking(num_bins=10, capacity=20, min_item_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_42\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | 25       |\n",
      "| time/              |          |\n",
      "|    fps             | 975      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 174        |\n",
      "|    ep_rew_mean          | 26.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01022795 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.53      |\n",
      "|    explained_variance   | 0.741      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.37       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    value_loss           | 5.06       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 175         |\n",
      "|    ep_rew_mean          | 28.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 665         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010599691 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 4.26        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 177         |\n",
      "|    ep_rew_mean          | 31.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010072157 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 178         |\n",
      "|    ep_rew_mean          | 33.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011987185 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    value_loss           | 3.11        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 180         |\n",
      "|    ep_rew_mean          | 34.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008360031 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.953       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 2.67        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 177         |\n",
      "|    ep_rew_mean          | 36.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009069365 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.548       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 175         |\n",
      "|    ep_rew_mean          | 38.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 628         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011086308 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.684       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 1.85        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 174         |\n",
      "|    ep_rew_mean          | 41.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008606775 |\n",
      "|    clip_fraction        | 0.0934      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.594       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 1.89        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 174          |\n",
      "|    ep_rew_mean          | 44.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 621          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075513073 |\n",
      "|    clip_fraction        | 0.0838       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.965        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.72         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0234      |\n",
      "|    value_loss           | 1.71         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1e80c5166d0>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPO_Path = os.path.join('Training', 'Saved Models', 'Constant_PPO_Model_Discard_Penalty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = bin_data(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:51\n",
      "{'placed': 57, 'misplaced': 6, 'discarded': 67}\n",
      "Episode:2 Score:56\n",
      "{'placed': 60, 'misplaced': 4, 'discarded': 92}\n",
      "Episode:3 Score:47\n",
      "{'placed': 56, 'misplaced': 9, 'discarded': 129}\n",
      "Episode:4 Score:51\n",
      "{'placed': 64, 'misplaced': 13, 'discarded': 137}\n",
      "Episode:5 Score:57\n",
      "{'placed': 60, 'misplaced': 3, 'discarded': 82}\n",
      "Episode:6 Score:56\n",
      "{'placed': 58, 'misplaced': 2, 'discarded': 69}\n",
      "Episode:7 Score:66\n",
      "{'placed': 71, 'misplaced': 5, 'discarded': 166}\n",
      "Episode:8 Score:53\n",
      "{'placed': 58, 'misplaced': 5, 'discarded': 99}\n",
      "Episode:9 Score:52\n",
      "{'placed': 57, 'misplaced': 5, 'discarded': 92}\n",
      "Episode:10 Score:56\n",
      "{'placed': 68, 'misplaced': 12, 'discarded': 156}\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 1000\n",
    "episodes = 10\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    steps = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done and steps < MAX_STEPS:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "    print('Episode:{} Score:{}'.format(episode,score))\n",
    "    print(env.logs)\n",
    "    real_data.log(env)\n",
    "    env.logs = { 'placed':0, 'misplaced':0, 'discarded':0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of steps taken: 387.6\n",
      "Average bin utilization: 98.64%\n",
      "Accuracy: 13.35%\n"
     ]
    }
   ],
   "source": [
    "control_data.print_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of steps taken: 176.2\n",
      "Average bin utilization: 98.64%\n",
      "Accuracy: 90.49%\n"
     ]
    }
   ],
   "source": [
    "real_data.print_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
